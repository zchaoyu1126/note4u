---
title: 《K8S in Action》第四章笔记
categories: Cloud Native
cover: 'http://img.note4u.top/base/cnnotes_cover.jpg'
abbrlink: 90f3ebae
date: 2023-04-19 21:20:27
tags:
hide: true
---

pod 中的 container 失败时，会怎样，会自动重启吗？？？？


在实际使用 K8S 进行部署时，一般不会直接创建 pod，而是使用 ReplicationController、Deployment 等资源实现对 pod 的创建与管理。

这些资源将自动监控 pod。

在本章中，你将了解 Kubernetes 如何检查容器是否仍然存在，如果不存在则重新启动容器。
你还将学到如何运行托管的pod，既可以无限期运行，也可以执行单个任务，然后终止运行。


## pod 探针

使用K8S的一个好处是，可以给K8S一个容器列表来由其保持容器在集群中的运行。可以通过让K8S创建pod资源，为其选择一个工作节点，并在该节点上运行该pod的容器来完成此操作。

如果其中一个容器终止？一个pod中的所有容器都终止？怎么办？

只要将pod调度到某个节点，该节点的kubelet就会运行 pod 的容器，从此只要该 pod 在，就会保持运行。

如果容器的主进程崩溃，Kubelet将会重启 容器 。

如果应用程序中有一个导致它每个一段时间就会崩溃的bug，k8s会自动重启应用程序。即使应用程序本身没有做任何特殊的事情，在k8s中运行也能获得自我修复的能力。

即使进程没有崩溃，有时应用程序也会停止正常工作。例如，具有内存泄漏的Java应用程序将开始抛出 OutOfMemoryErrors，但JVM进程会一直运行。

让应用程序向 K8S发出信号，告诉它运行异常，让其重启。--》一个崩溃的容器会自动重启，所以捕获错误，并在错误发生时推出进程。但这并不能解决所有问题。


你的应用因为无限循环或死锁而停止响应。为了确保应用程序在这种情况下可以重新启动，必须从外部检查应用程序的运行状况，而不是依赖应用的内部检测。

### 存活探针
K8S 可以通过存活探针（liveness probe）检查容器是否还在运行。
可以为pod中的每个容器单独指定存活探针。如果探测失败，K8S 将定期执行探针并重新启动容器。

- HTTP GET探针对容器的IP地址（你指定的端口和路径）执行HTTP GET请求。
- TCP 套接字探针尝试与容器指定端口建立 TCP 连接。
- Exec 探针在容器内执行任意命令，并检查命令的退出状态码。 如果状态码是0，则探测成功。

### 基于 HTTP GET 请求的存活探针

``` SHELL
kubectl create -f kubia-liveness.yaml
```

``` YAML
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness-pod
spec:
  containers:
  - name: kubia-liveness-container
    image: luksa/kubia-unhealthy
    resources:
      limits:
        memory: "128Mi"
        cpu: "500m"
    ports:
      - containerPort: 8080
        protocol: TCP
    livenessProbe:
      httpGet:
        path: /
        port: 8080
```

![20230420170617](http://img.note4u.top/article/20230420170617.png)


kubectl logs kubia-liveness-pod --previous
Kubia server starting...
Received request from ::ffff:10.244.0.1
Received request from ::ffff:10.244.0.1
Received request from ::ffff:10.244.0.1
Received request from ::ffff:10.244.0.1
Received request from ::ffff:10.244.0.1
Received request from ::ffff:10.244.0.1
Received request from ::ffff:10.244.0.1
Received request from ::ffff:10.244.0.1

kubectl logs kubia-liveness-pod
Kubia server starting...
Received request from ::ffff:10.244.0.1
Received request from ::ffff:10.244.0.1
Received request from ::ffff:10.244.0.1



Name:             kubia-liveness-pod
Namespace:        default
Priority:         0
Service Account:  default
Node:             minikube/192.168.49.2
Start Time:       Thu, 20 Apr 2023 11:24:47 +0800
Labels:           <none>
Annotations:      <none>
Status:           Running
IP:               10.244.0.30
IPs:
  IP:  10.244.0.30
Containers:
  kubia-liveness-container:
    Container ID:   docker://dda81f77e9bf5526ab0e22f0b68f70bb5cc70c41dd4051c73e3c5ea9fb37ec27
    Image:          luksa/kubia-unhealthy
    Image ID:       docker-pullable://luksa/kubia-unhealthy@sha256:5c746a42612be61209417d913030d97555cff0b8225092908c57634ad7c235f7
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 20 Apr 2023 17:07:26 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Thu, 20 Apr 2023 17:05:30 +0800
      Finished:     Thu, 20 Apr 2023 17:07:20 +0800
    Ready:          True
    Restart Count:  34
    Limits:
      cpu:     500m
      memory:  128Mi
    Requests:
      cpu:        500m
      memory:     128Mi
    Liveness:     http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pxfr6 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-pxfr6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason          Age                      From     Message
  ----     ------          ----                     ----     -------
  Warning  Unhealthy       3h58m (x79 over 5h39m)   kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500
  Normal   Pulling         3h48m (x30 over 5h43m)   kubelet  Pulling image "luksa/kubia-unhealthy"
  Warning  BackOff         3h43m (x308 over 5h27m)  kubelet  Back-off restarting failed container kubia-liveness-container in pod kubia-liveness-pod_default(985969a9-4bdf-49f4-9498-e1b064c97b11)
  Normal   SandboxChanged  5m41s                    kubelet  Pod sandbox changed, it will be killed and re-created.
  Normal   Pulled          5m7s                     kubelet  Successfully pulled image "luksa/kubia-unhealthy" in 10.91591173s (10.915936531s including waiting)
  Normal   Pulled          3m4s                     kubelet  Successfully pulled image "luksa/kubia-unhealthy" in 4.767803682s (4.767815982s including waiting)
  Normal   Killing         101s (x2 over 3m41s)     kubelet  Container kubia-liveness-container failed liveness probe, will be restarted
  Normal   Pulling         70s (x3 over 5m18s)      kubelet  Pulling image "luksa/kubia-unhealthy"
  Normal   Pulled          67s                      kubelet  Successfully pulled image "luksa/kubia-unhealthy" in 3.216375977s (3.216382977s including waiting)
  Normal   Created         66s (x3 over 5m4s)       kubelet  Created container kubia-liveness-container
  Normal   Started         65s (x3 over 5m2s)       kubelet  Started container kubia-liveness-container
  Warning  Unhealthy       1s (x8 over 4m1s)        kubelet  Liveness probe failed: HTTP probe failed with statuscode: 500


laststate:137  
表示该进程由外部信号终止，128+x, x是终止进程的信号编号。 x=9 SIGKILL
底部列出的事件显示了容器为什么终止，K8S发现容器不健康，所以终止并重新创建。
当容器被强行终止时，会创建一个全新的容器，而不是重启原来的容器。


Liveness:     http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3

delay=0s 部分显示在容器启动后立即开始探测。timeout仅设置为1秒，因此容器必须在1秒内进行响应，不然这次探测记作失败，
每10秒探测一次容器（period=10s)，并在探测连续三次失败（#failure=3)后重启容器

定义探针时，可以自定义这些附加参数。
livenessProbe:
  httpGet:
    path: /
    port: 8080
  initialDelaySeconds: 15

如果没有设置初始盐池，探针将在启动时立即开始探测容器，这通常会导致探测失败，因为应用程序还没来得及准备好开始接受请求。
如果失败次数超过阈值，那么在应用程序能够正确响应请求之前，容器就会重启。

很多场合都会看到这种情况，用户很困惑为什么他们的容器正在重启。但是如果使用 kubectl describe，他们会看到容器以退出码 137 或 143 结束，并告诉他们该pod是被迫终止的。
此外，pod 事件的列表将显示容器因 liveness 探测失败而被终止。如果你在pod启动时看到这种情况，那是因为未能适当设置 initialDelaySeconds

137=128+9
143=128+15(SIGTERM)

### 创建有效的存活探针
对于正在生产运行中的pod，一定要定义一个存活探针，如果没有探针的话，k8s无法知道你的应用是否还活着。只要进程还在运行，k8s会认为容器是健康的。

1 存活探针应该检查什么？？
简易的存活探针仅仅检查了服务器是否响应，虽然这看起来简单，但是这在大多数情况下已经足够。
为了更好地进行存活检查，需要将探针配置为请求特定的URL路径，例如 /health 并让应用从内部运行的所有重要组件执行状态检查，以确保它们都没有终止或停止响应。

请确保/health HTTP端点不需要认证，否则探测会一直失败，导致容器无限重启。

一定要检查应用程序的内部，而没有任何外部因素的影响。例如，当服务器无法连接到后端数据库时，前端web服务器的存活探针不应该返回失败。
如果问题的底层原因在数据库，重启web服务器并不会解决该问题。由于存活探测的原因，将反复重启容器直到数据库恢复。

2 保持探针轻量
存活探针不应该消耗太多的计算资源，并且运行不应该花太长时间。
默认情况下，探测器执行的频率相对较高，必须在一秒内执行完毕。
一个过重的探针会大大减慢服务器的运行时间。

限制容器可用的CPU时间，探针的CPU时间计入容器的CPU时间配额，因此重量级的存活探针，将减少主应用程序的可用CPU时间。

如果在容器中运行java应用程序，请确保使用HTTP GET存活探针，而不是启动全新JVM以获取存活信息的 Exec探针。任何基于JVM或类似的应用程序也是如此，因为它们的启动过程需要大量的计算资源。

3 无须在探针中实现重试循环

探针的失败阈值是可以配置的，并且通常在容器被终止之前探针必须失败多次。
但即使你将失败阈值设置为1， Kubernetes为了确认一次探测的失败，会尝试若干次。因此在探针中自己实现重试循环是浪费精力。

## ReplicationController
### 概念
ReplicationController 可以创建和管理一个 pod 的多个副本，并会持续监控管理的所有 pod，保证相应其与期望的数目相等。

{% note info flat %}
在下文描述中，将使用 rc 来代替 ReplicationController。
{% endnote %}

rc 主要由三个部分组成：
- 标签选择器：用于确定 rc 作用域中有哪些 pod
- 副本个数：指定应运行的 pod 数量
- pod 模板：用于创建新的 pod 副本

当 rc 创建完毕后，标签选择器、副本个数、pod模板都可以随时修改，但只有副本数目的变更会影响现有的 pod。更改标签选择器，会使得现有的 pod 脱离 rc 的管理范围。而修改 pod 模板，则会影响由 rc 新创建的 pod。具体内容将在后文的小节中，结合实例进行说明。


使用 rc 的好处：
- 确保一个 pod 或多个 pod 副本的持续运行
- 集群节点发生故障时，将为故障节点上运行的所有 pod 创建替代副本
- 能够轻松实现 pod 的（手动、自动）水平伸缩

### 创建 rc

``` yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  template:
    metadata:
      name: kubia-pod
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia-container
        image: zchaoyu1126/kubia
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
          protocol: TCP
```
{% note warning flat %}
使用原文中的 yaml 配置，出现 ImagePullBackOff、ErrImagePull 等错误，加入 resources 等信息后成功运行。
此外，关于 pod、container 等名字，应该如何命名？
{% endnote %}

pod 模板中的 pod 标签必须和 rc 的标签选择器匹配，否则 rc 将无休止地创建新容器。因为启动新的 pod 不会使实际的副本数量接近期望的副本数量。为了防止出现这种状况，API 服务会校验 rc 的定义，不会接受错误配置。

{% note info flat %}
不指定标签选择器时，会自动根据 pod 模板中的标签进行自动配置，这样可以使 yaml 文件更简短。
{% endnote %}

``` shell
kubectl create -f kubia-rc.yaml
kubectl get pods --show-labels
```

![20230515214356](http://img.note4u.top/article/20230515214356.png)

### pod 删除 
删除由 rc 管理的 kubia-4kb6x，此时 rc 将通过创建一个新的替代 pod 来响应 pod 的删除操作。

``` shell
kubectl delete pod kubia-4kb6x
kubectl get pods --show-labels
```

![20230515214542](http://img.note4u.top/article/20230515214542.png)

![20230515214836](http://img.note4u.top/article/20230515214836.png)

{% note warning flat%}
这里的 ErrImagePull 是什么？ 后面再次获取 pod 列表，为什么又自己好了？
{% endnote %}

通过 `get` 查看 rc，从左到右的数字依次表示所需的、实际的、就绪的 pod 数目。有时实际的 pod 数目会大于所需的 pod 数，这是因为存在一些 pod 正处于删除等状态。使用 `describe` 可以获取更详细的信息。

``` shell
kubectl get rc kubia
kubectl describe rc kubia
```

![20230515214653](http://img.note4u.top/article/20230515214653.png)

![20230515215315](http://img.note4u.top/article/20230515215315.png)

### 节点下线

当一个节点在几分钟内无法访问时，rc 会将调度到该节点的 pod 状态将变为`Unkonwn`，并立即启动一个新的 pod 代替。当节点再次启动时，对应状态应该返回到 `Ready`，并且状态为 `Unkonwn` 的 pod 将被删除。

{% note warning flat %}
使用minikube 无法完成本实验，之后有条件时验证。
{% endnote %}

### 从 rc 的作用域中移除
由 rc 创建的 pod 并不是与 rc 绑定。在任何时刻，rc 管理与标签选择器匹配的 pod。更改 pod 的标签，可以将它从 rc 的作用域中添加或删除，甚至可以从一个移动到另一个。

``` shell
kubectl label pod kubia-manual-gpu app=kubia --overwrite
kubectl get pods --show-labels
kubectl label pod kubia-manual-gpu app=foo --overwrite
kubectl get pods --show-labels
```

![20230515220320](http://img.note4u.top/article/20230515220320.png)

![20230515220407](http://img.note4u.top/article/20230515220407.png)

尽管 pod 没有绑定到 rc,但在 pod 的 metadata.ownerReferences 字段中引用了 rc，可以轻松使用该字段来找到 pod 所属的 rc。

``` shell
kubectl get pod kubia-t9bpq -o yaml
```

![20230515220919](http://img.note4u.top/article/20230515220919.png)

### 修改标签选择器

如果修改 rc 的标签选择器，让所有的pod都脱离 rc 的控制，导致它创建三个新的pod。k8s 确实允许更改 rc 的标签选择器，但修改标签选择器的同时，必须修改 pod 模板。此外，这种做法不适用于本章后半部分中介绍的其他资源。

### 修改 pod 模板

rc 的 pod 模板可以随时修改，但不能对原有匹配的标签进行修改。否则，需要同时修改标签选择器。此时，会重新生成对应数目的 pod。

pod 模板修改完毕后，但不会影响现有的 pod。只有将旧的 pod 删除后，rc 才会根据新模板将其替换为新的pod。

![20230515093701](http://img.note4u.top/article/20230515093701.png)

找到pod模板部分并向元数据添加一个新的标签。然后再次列出 pod 及其标签，发现标签并未发生变化。但如果删除了某个 pod 后，并等待其替代 pod 创建，就会发现新的标签。

![20230515222441](http://img.note4u.top/article/20230515222441.png)

![20230515222620](http://img.note4u.top/article/20230515222620.png)

像这样首先编辑 rc ，来更改容器模板中的容器图像。其次，删除现有的容器，最后将它们替换为新模板中的新容器，可以用于升级 pod。但实际使用时，常使用 Deployment 完成。

可以设置 KUBE_EDITOR 环境变量来告诉 kubectl 使用期望的文本编辑器，否则使用默认文本编辑器。

### 修改副本数目

修改 spec.replicas 字段（副本数目），可以实现水平扩容或缩容。

``` shell
kubectl edit rc kubia
kubectl scale rc kubia --replicas=10
kubectl scale rc kubia --replicas=3
``` 

### 删除 rc

删除 rc 时，由其管理的 pod 也会被删除，可以使用 `--cascade=false` 来保留 pod。
如果不输入该参数，则默认为 `--cascade=true`。

``` shell
kubectl delete rc kubia --cascade=false
```

## ReplicaSet
### 概念
ReplicaSet 是新一代的 rc，rc最终将被弃用。

{% note info flat %}
在下文描述中，将使用 rs 来代替 ReplicaSet
{% endnote %}

rc 和 rs 几乎完全相同，但通常不会直接创建 rc 或 rs，而是在创建更高层级的Deployment资源时自动创建。

rc 和 rs 的行为几乎完全相同，但 rs 的 pod 选择器表达能力更强。该部分内容，将在后续介绍。

### 创建 rs
创建一个名为 kubia-rs.yaml 的新文件，将你的Kind 从
replicationcontroller 修改为 replicaset

``` yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      name: kubia-pod
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia-container
        image: zchaoyu1126/kubia
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 8080
          protocol: TCP
```

``` shell
kubectl create -f kubia-rs.yaml
kubectl get rs kubia
kubectl get pods --show-labels
```

### 标签选择器
rs 相对于 rc 的主要改进是更具表达力的标签选择器。rs 除 matchLabels 外，还有更加强大的 matchExpressions 属性。在 matchExpressions 下的内容被称为表达式，表达式中必须包含一个 key、一个 operator，并且可能还有一个 values 的列表。

operator 的种类与作用：
- In: Label的值必须与其中一个指定的 values 匹配
- NotIn: Label的值与任何指定的 values 不匹配
- Exists: pod 必须包含一个指定的标签（值不重要）。使用此运算符时，values 属性不得指定。
- DoesNotExists: pod 不得包含指定名称的标签。 使用此运算符时，values 属性不得指定。

``` yaml
selector:
  matchLabels:
        app: kubia
```

``` yaml
selector:
  matchExpressions:
    - key: app
      operator: In
      values:
        - kubia
```

如果指定了多个表达式，则所有这些表达式都必须 true 才能使选择器与 pod 匹配。如果同时指定 matchLabels 和 matchExpressions，则所有标签都必须匹配，并且所有表达式必须计算为 true 以使该 pod 与该选择器匹配。

## DaemonSet

rc 和 rs 都是用于在 k8s 集群上运行部署特定数目的 pod。但是，当你希望 pod 在集群中的每个节点上运行时（并且每个节点都正好需要一个运行的pod实例时）。

每个节点上运行日志收集器和资源监控器
kube-proxy 运行在所有节点上才能使服务工作
![20230516145540](http://img.note4u.top/article/20230516145540.png)

DaemonSet 确保创建足够的 pod，并在自己的节点上部署每个 pod
确保一个pod 匹配的它选择器，并且在每个节点上运行？？？


如果节点下线，DaemonSet 不会再其他地方重新创建 pod。但是，当将一个新节点添加到集群中时，DaemonSet会立刻部署一个新的 pod 实例。
如果有人删除了一个，也会重新创建一个新的。
从配置的pod模板创建。


DaemonSet 将 pod 部署到集群的所有节点上，除非指定这些 pod 只在部分节点上运行。 这是通过 pod 模板中的 nodeSelector 属性指定的，
这是 DaemonSet 定义的一部分。


在本书的后面，你将了解到节点可以被设置为不可调度的，防止 pod 被部
署到节点上，DaemonSet 甚至会将 pod 部署到这些节点上，因为无法调度的属性只会被调度器使用，而 DaemonSet 管理 pod 则完全绕过调度器。

这是预期的，因为DaemonSet 的目的是运行 系统服务，即使是在不 可调度的节点上，系 统服务通常也需要运行


## Job

### Job 的功能
rc、 rs、 DaemonSet 会持续运行任务，永远达不到完成态。由上述管理的 pod 中的进程在退出时会重新启动。但是在一个可完成的任务中，在其进程终止后不应该再重新启动。因此，k8s 允许运行一种 pod，该 pod 在内部进程成功结束时，不重启容器，一旦任务完成，pod 就被认为处于完成状态。如果节点故障时，该节点上由 Job 管理的 pod 将被重新安排到其他节点；如果进程本身异常退出，则会重新启动。

### 创建 Job
``` shell
kubectl create -f exporter.yaml
kubectl get pods --show-labels
kubectl get job
```

``` yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  backoffLimit: 5
  activeDeadlineSeconds: 300
  ttlSecondsAfterFinished: 100
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job
```

### Job 的终止与清理
Job 终止的情况分为：
- 任务正常结束
- pod 失败重试次数超出 `.spec.backoffLimit` 上限
  默认情况下，Job 会持续运行，除非某个 Pod 失败（restartPolicy=Never） 或者某个容器出错退出（restartPolicy=OnFailure）。 这时，Job 基于 `.spec.backoffLimit` 来决定是否以及如何重试。 一旦重试次数到达 `.spec.backoffLimit` 所设的上限，Job 会被标记为失败， 其中运行的 Pod 都会被终止。
- 超出活跃 Job 的活跃期限，超出 `.spec.activeDeadlineSeconds`
  一旦 Job 的运行时间达到 activeDeadlineSeconds 秒，其所有运行中的 Pod 都会被终止， 并且 Job 的状态更新为 type: Failed 及 reason: DeadlineExceeded。

{% note info flat %}
1. Job 的 `.spec.activeDeadlineSeconds` 优先级高于其 `.spec.backoffLimit` 设置。因此，如果一个 Job 正在重试一个或多个失效的 Pod，该 Job 一旦到达 activeDeadlineSeconds 所设的时限即不再部署额外的 Pod， 即使其重试次数还未达到 backoffLimit 所设的限制。
2. Job 规约和 Job 中的 Pod 模板规约 都有 activeDeadlineSeconds 字段。 请确保在合适的层次设置正确的字段
{% endnote %}

由于 `.spec.activeDeadlineSeconds` 和 `.spec.backoffLimit` 所触发的 Job 终结机制都会导致 Job 永久性的失败，而这类状态都需要手工干预才能解决。

当你的 Job 已结束时，将 Job 保留在 API 中（而不是立即删除 Job）很有用， 这样就可以判断 Job 是成功还是失败。此外，k8s 的TTL-after-finished 控制器提供了一种 TTL 机制来限制已完成执行的 Job 对象的生命期。仅需设置 `.spec.ttlSecondsAfterFinished` 字段即可。上述示例中的 Job 将在结束 100 秒之后，可以成为被自动删除的对象。如果该字段设置为 0，Job 在结束之后立即成为可被自动删除的对象。如果该字段没有设置，Job 不会在结束之后被 TTL 控制器自动清除。

![20230516152554](http://img.note4u.top/article/20230516152554.png)

### 在 Job 中运行多个实例

``` yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5
  template:

```

``` yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5
  parallelism: 2
    template:
```

通过以下命令行，可以修改并行数量，由 2 改为 3。

``` shell
kubectl scale job multi-completion-batch-job --replicas 3
```

## CronJob
### 创建 CronJob
CronJob 创建基于时隔重复调度的 Job，常用于执行排期操作，例如备份、生成报告等。

``` yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: batch-job-every-fifteen-minutes
spec:
  schedule: "0,15,30,45 * * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: periodic-batch-job
        spec:
          restartPolicy: OnFailure
          containers:
          - name: main
            image: luksa/batch-job
```

### Cron 时间表语法
`.spec.schedule` 字段是必需的。该字段的值遵循 Cron 语法：

```
# ┌───────────── 分钟 (0 - 59)
# │ ┌───────────── 小时 (0 - 23)
# │ │ ┌───────────── 月的某天 (1 - 31)
# │ │ │ ┌───────────── 月份 (1 - 12)
# │ │ │ │ ┌───────────── 周的某天 (0 - 6)（周日到周一；在某些系统上，7 也是星期日）
# │ │ │ │ │                                或者是 sun，mon，tue，web，thu，fri，sat
# │ │ │ │ │ 
# │ │ │ │ │
# * * * * *
```

可以用步长进行范围组合，范围后面带有 /<数字> 可以声明范围内的步幅数值。因此每两小时的表示方式可以是：
- 0-23/2 
- */2
- 0,2,4,6,8,10,12,14,16,18,20,22

除了标准语法，还可以使用一些类似 @monthly 的宏：
- @yearly 或 @annually：每年 1 月 1 日的午夜运行一次，即 0 0 1 1 *
- @monthly：每月第一天的午夜运行一次，即 0 0 1 * *
- @weekly：每周的周日午夜运行一次，即 0 0 * * 0
- @daily 或 @midnight：每天午夜运行一次，即 0 0 * * *
- @hourly：每小时的开始一次，即 0 * * * *


### 任务模板
`.spec.jobTemplate` 为 CronJob 创建的 Job 定义模板，它是必需的。它和 Job 的语法完全一样， 只不过它是嵌套的，没有 apiVersion 和 kind。可以为模板化的 Job 指定通用的元数据，例如标签或注解。

### 任务延迟开始的最后期限
`.spec.startingDeadlineSeconds` 字段是可选的。它表示任务如果由于某种原因错过了调度时间，开始该任务的截止时间的秒数。

过了截止时间，CronJob 就不会开始该任务的实例（未来的任务仍在调度之中）。例如，如果有一个每天运行两次的备份任务，可能会允许它最多延迟 8 小时开始，但不能更晚，因为更晚进行的备份将变得没有意义，宁愿等待下一次计划的运行。对于错过已配置的最后期限的 Job，Kubernetes 将其视为失败的任务。如果没有为 CronJob 指定 startingDeadlineSeconds，那 Job 就没有最后期限。

如果 `.spec.startingDeadlineSeconds` 字段被设置（非空），CronJob 控制器将会计算从预期创建 Job 到当前时间的时间差。如果时间差大于该限制，则跳过此次执行。例如，如果将其设置为 200，则 Job 控制器允许在实际调度之后最多 200 秒内创建 Job。

### 并发性规则
`.spec.concurrencyPolicy` 也是可选的。它声明了 CronJob 创建的任务执行时发生重叠如何处理。 spec 仅能声明下列规则中的一种：

Allow（默认）：CronJob 允许并发任务执行。
Forbid： CronJob 不允许并发任务执行；如果新任务的执行时间到了而老任务没有执行完，CronJob 会忽略新任务的执行。
Replace：如果新任务的执行时间到了而老任务没有执行完，CronJob 会用新任务替换当前正在运行的任务。

{% note info flat %}
并发性规则仅适用于同一个 CronJob 创建的任务。如果有多个 CronJob，它们相应的任务总是允许并发执行的。
{% endnote %}